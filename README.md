# Web Scraping

## About This Course
As a Data Scientist, one is responsible for crunching humongous amounts of data to extract insights and streamline businesses based on the results. But the role of a Data Scientist doesn't start with understanding and analyzing data. Before we do any analysis, we must have data at hand. The first step to solving any data problem is to identify the problem, followed by collecting relevant data, and cleaning and representing the data in a functional form. Then we can use visualization and other analytical techniques to glean any useful insights.

It is fundamentally essential that data scientists can collect data from various sources. Data could be available in a structured form via well-defined REST APIs or unstructured (raw) data from websites, and any other type of data in-between. The Web Scraping course is all about extracting data of interest from any source.

The course will be divided into 5 parts. The first part deals with the basics of Python, which is completely optional for students with prior experience using Python. However, I recommend taking a quick glance at it unless you use Python on a day-to-day basis. The second part of the course deals with advanced Python coding necessary for web scraping. The third deals with extracting structured data using APIs. In the fourth part, we throw light on basic tools and packages of Python for web and Chrome development tool. Our fifth and final part deals with extracting raw data from web pages using Scrapy package.

NOTE: All the lectures are written in iPython/Jupyter notebooks. These files cannot be viewed on Canvas, and even on your computer, a Jupyter server has to be enabled to view these files. Therefore, for convenience, they have been compiled into HTML files. The only downside with HTML files is you cannot run the code there. So to read and execute at the same time, use iPython notebooks. For a quick read, go for HTML or do as you please.

## Course Goals and Roadmap
By the end of this course, students will be able to extract/scrape data from a variety of sources using APIs or writing spiders to crawl a website. The course will be hands-on, giving the student an opportunity to explore and scrape any type of data of their choice.

### Part 1: Fundamentals of Python (Optional)
Using iPython notebooks
Control flow
Functions
Data Structures: Lists, tuples, dictionaries
Iterables and generators

### Part 2: Essentials of Python
Object-oriented programming using Python
Error and Exception handling
File Input / Output
CSV files
JSON files
Strings and Regular Expressions

### Part 3: Structured Data Extraction
REST APIs
Twitter API

### Part 4: Fundamentals of Web Data and Developer Tools
HTML
XML
Chrome dev-tools
urllib package
BeautifulSoup package

### Part 5: Building Spiders using Scrapy
Scrapy package
